# Projector–Camera Control System

<img src="GUI-Screenshot.png" width="100%">  

A Python + PyQt GUI application for controlling an Allied Vision camera (via **VmbPy / VimbaX**) and a projector (second display in fullscreen mode), supporting structured-light pattern projection, synchronized image capture, camera–projector calibration, and offline 3D reconstruction.  

```
┌─────────────────────────────────────────────────────────────────────┐
│                   Projector–Camera Control System                   │
└─────────────────────────────────────────────────────────────────────┘
             │
             ▼
      [GUI] python main.py
             │
             ▼
      Start Streaming (Live Preview)
             │
             ▼
   Open Projector Window (Fullscreen)
             │
             ▼
        Choose Pipeline
     ┌───────┴───────────────────────────────┐
     │                                       │
     ▼                                       ▼
(A) Calibration Capture                  (B) Experiment Capture
Save → calibration_capture/              Save → projection_exp_capture/
      <CaptureID>/<session>/                   <CaptureID>/<session>/
     │                                       │
     ▼                                       ▼
Offline: proj_cam_calibration.ipynb      Offline: traditional_3d_reconstruction.ipynb
     │                                       │
     ├──────────────┐                        │
     ▼              │                        ▼
Outputs:            │              Uses calibration results:
- camera intrinsics │              - intrinsics / extrinsics
- extrinsics        │
- final summary     │                        ▼
- checkerboard poses└────────────────> Reconstruction Outputs:
                                   - decoded correspondences
                                   - pointcloud_camera / pointcloud_world
                                   - interpolated maps (.npy)
```

---

## Highlights

* **GUI-based live preview** with basic camera controls (exposure, gain, pixel format, binning).
* **Fullscreen projector pattern window** (intended for a secondary monitor / projector).
* **Projection capture workflow**:

  * Select a **pattern folder** (Gray code, shifted bit-0, or customized images)
  * Capture **multiple frames per pattern** (burst mode), saved into a structured session directory
  * Optional **capture averaging** to reduce sensor noise
* **Offline processing tools included**:

  * Gray-code decoding utilities under `traditional_3d_reconstruction/`
  * Camera–projector calibration and reconstruction helpers under `projector_camera_calibration/`

---

## Repository Layout

### Core application

* `main.py` — **GUI entry point**
* `viewer.py` — main PyQt window, live image viewer, and control panels
* `dialogs.py` — embedded configuration, calibration, and capture workflows
* `config.py` — default camera, projector, and capture settings
* `streaming.py` — camera streaming thread and buffer management
* `camera_feature_manager.py`, `camera_utils.py` — camera control wrappers (VmbPy)
* `project_pattern.py` — projector window and fullscreen pattern display
* `plot_1d_profile.py` — interactive 1D intensity profile tool
* `utils.py`, `global_state.py` — shared helpers and global state

### Patterns

* `gray_code_patterns/` — example Gray-code and shifted bit-0 patterns (PNG)

### Calibration and reconstruction

* `projector_camera_calibration/`

  * `calib_recon_utils.py` — calibration and reconstruction utilities
  * `proj_cam_calibration.ipynb` — camera–projector calibration notebook
* `traditional_3d_reconstruction/`

  * `decode_pattern.py` — Gray-code decoding utilities
  * `traditional_3d_reconstruction.ipynb` — reconstruction notebook

### Captured data (generated by the GUI)

* `calibration_capture/<date>/...` — **used for calibration**
* `projection_exp_capture/<date>/...` — **used for reconstruction**

---

## System Requirements

### OS

* Windows recommended (typical VimbaX / VmbPy workflow).
  Most code is cross-platform **except camera SDK components**.

### Hardware

* A projector or secondary display (used for fullscreen pattern projection)
* Allied Vision camera supported by **VimbaX + VmbPy**

### Python dependencies (typical)

* `PyQt5`
* `numpy`
* `opencv-python` (or `opencv-contrib-python` if needed)
* `matplotlib`
* `scipy`
* `tqdm`
* `vmbpy`
* *(optional, for point-cloud processing)* `open3d`

> ⚠️ `vmbpy` requires a working **Allied Vision VimbaX** installation and drivers.

---

## Installation (recommended: Mamba)

```bash
mamba create -n projcam python=3.10 -y
mamba activate projcam

pip install PyQt5 numpy opencv-python matplotlib scipy tqdm
pip install vmbpy
# optional
pip install open3d
```

If you are using VimbaX:

* Install the Allied Vision VimbaX Runtime / Drivers
* Verify that the camera is visible in Allied Vision tools before launching the GUI

---

## Quickstart (How you REALLY run it)

### 1) Launch the GUI

From the repository root:

```bash
python main.py
```

This opens the main window (e.g. **Projector + Camera Control System**).

---

### 2) Start camera preview (live streaming)

In the GUI:

1. Configure exposure, gain, pixel format, and binning if needed
2. Click **Start Streaming**

You should see the live camera feed updating in the viewer.

> If streaming fails, the issue is usually related to VimbaX/VmbPy installation, camera permissions, or another application holding the camera.

---

### 3) Open the projector pattern window

Pattern projection is handled by `project_pattern.py`.

Typical usage:

* Enable the projector display window from the GUI
* The pattern window opens fullscreen on the selected projector / monitor

**Tips**

* Ensure Windows display mode is set to **Extend**
* Use the projector as the secondary display
* If patterns appear on the wrong screen, adjust monitor selection in the GUI or configuration

---

## Pattern Selection (Gray code vs. customized images)

The GUI calibration panel allows selecting a **pattern folder**.

In the embedded calibration panel (`dialogs.py`):

* **Pattern folder**: directory containing PNG/JPG patterns
* **Browse…**: select a directory
* **Use Customized Pattern**: enable custom pattern capture
* **Customized Pattern Folder**: directory containing a user-defined pattern sequence

### Example patterns included

* `gray_code_patterns/` contains:

  * `HorGrayCode_XX.png` and inverses
  * `VerGrayCode_XX.png` and inverses
  * shifted bit-0 patterns (e.g. `HorBit0_Shifted_0.png`)
  * `WhitePattern.png`, `BlackPattern.png`

The capture pipeline automatically enumerates and sorts valid image files in the selected folder.

---

## End-to-end Workflow

This repository implements **two distinct capture pipelines**, each serving a different purpose:

* **`calibration_capture/`**
  Used for **camera–projector calibration** (intrinsics, extrinsics, checkerboard poses, etc.)

* **`projection_exp_capture/`**
  Used for **actual measurement and reconstruction experiments**
  (capturing structured-light patterns on objects and reconstructing geometry)

Understanding this separation is critical for correct usage.

---

## Calibration Capture Workflow (GUI)

This workflow generates data for **projector–camera calibration**.

### 1) Open the Projection panel

In the main GUI, open the panel created by:

* `create_projector_calibration_panel(...)`

Available controls typically include:

* **Capture Save Folder**
* **Capture ID**
* **Burst Count**
* **Auto Reset Index**
* **Start Projection Capture**
* **Average Captures**
* **Pattern folder selectors**

---

### 2) Set output location

By default, captures are saved to:

```
<repo_root>/calibration_capture
```

(as defined in `config.py`).
This can be changed in the GUI.

---

### 3) Set Capture ID and session index

* **Capture ID**: usually a date string (e.g. `12192025`)
* Output folders are created as:

```
calibration_capture/<CaptureID>/<session_index>/
```

Example:

```
calibration_capture/12192025/1/
calibration_capture/12192025/29/
```

If **Auto Reset Index** is enabled, the session index resets when the Capture ID changes.

---

### 4) Set Burst Count

* Number of frames captured per projected pattern
* Files are saved as:

```
<pattern_stem>_00.png
<pattern_stem>_01.png
...
```

---

### 5) Start Projection Capture

For each pattern:

1. The pattern is displayed fullscreen on the projector
2. The camera captures `burst_count` frames
3. Images are saved into the session directory

---

## Output Folders (Critical)

### 1) Calibration data

Generated during calibration capture:

```
calibration_capture/<CaptureID>/<session_index>/
```

Contains raw calibration captures and calibration results such as:

* `*_camera_calibration_results.json / .npz`
* `*_final_calibration_summary.json`
* `*_checkerboard_poses_plotly.json`
* `visualize_corners_projector_*.png`

---

### 2) Experiment data (for reconstruction)

Generated during experiment capture:

```
projection_exp_capture/<CaptureID>/<session_index>/
```

Contains object measurements and reconstruction outputs:

* `pointcloud_camera.npy / .ply`
* `pointcloud_world.npy / .ply`
* interpolated maps:

  * `interp_depth_cam.npy`
  * `interp_height_world.npy`
  * `interp_rgb_albedo_*.npy`

---

## Camera–Projector Calibration (Offline)

Calibration must be completed **before reconstruction**.

### Calibration notebook

Open:

* `projector_camera_calibration/proj_cam_calibration.ipynb`

Typical steps:

1. Select a calibration session folder, e.g.

   ```
   calibration_capture/12192025/29/
   ```
2. Perform camera calibration (intrinsics, distortion)
3. Perform projector calibration and solve camera–projector geometry
4. Export calibration summaries:

   * `<CaptureID>_camera_calibration_results.json / .npz`
   * `<CaptureID>_final_calibration_summary.json`
   * `<CaptureID>_checkerboard_poses_plotly.json`

---

## Offline Reconstruction (How to run it)

Offline reconstruction uses **experiment data**, not calibration data.

Open:

* `traditional_3d_reconstruction/traditional_3d_reconstruction.ipynb`

### Typical workflow

1. Select an experiment session folder, e.g.

   ```
   projection_exp_capture/12192025/19/
   ```
2. Load captured Gray-code and inverse images
3. Decode projector correspondences using:

   * `traditional_3d_reconstruction/decode_pattern.py`
4. Use calibration results from the previous step to reconstruct:

   * camera-frame point clouds
   * world-frame point clouds (if extrinsics are available)
5. Optionally interpolate reconstructed data to dense maps:

   * depth, height, RGB albedo

> **Key dependency**:
> Calibration parameters come from `calibration_capture/`,
> while reconstruction images come from `projection_exp_capture/`.

---

## Common Issues

### Camera not detected / streaming fails

* Verify VimbaX installation and drivers
* Close other applications using the camera
* Confirm `vmbpy` works in the active environment

### Projector fullscreen on wrong monitor

* Ensure display mode is **Extend**
* Adjust projector screen selection in the GUI or configuration

---

## How to Cite / Acknowledge

If you publish results using this system, please cite your structured-light methodology and reference this repository as the control and capture pipeline.

---

## Author

**Xiao Wang**
[xwang3@arizona.edu](mailto:xwang3@arizona.edu)
